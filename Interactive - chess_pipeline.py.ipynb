{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to dlt-example-zHJnWXT8-py3.12 (Python 3.12.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432dafd-3824-410c-8101-045d690394fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline chess_pipeline load step completed in 13.86 seconds\n",
      "1 load package(s) were loaded to destination databricks and into dataset chess_players_games_data\n",
      "The filesystem staging destination used az://jycontainer location to stage data\n",
      "The databricks destination used <dlt.common.configuration.specs.azure_credentials.AzureCredentialsWithoutDefaults object at 0x7fc6a62a8f20> location to store data\n",
      "Load package 1730852146.2935328 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "from chess import source\n",
    "\n",
    "\n",
    "def load_players_games_example(start_month: str, end_month: str) -> None:\n",
    "    \"\"\"Constructs a pipeline that will load chess games of specific players for a range of months.\"\"\"\n",
    "\n",
    "    # configure the pipeline: provide the destination and dataset name to which the data should go\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"chess_pipeline\",\n",
    "        destination=\"databricks\",\n",
    "        dataset_name=\"chess_players_games_data\",\n",
    "        staging=dlt.destinations.filesystem(\"az://jycontainer\")\n",
    "    )\n",
    "    # create the data source by providing a list of players and start/end month in YYYY/MM format\n",
    "    data = source(\n",
    "        [\"magnuscarlsen\", \"vincentkeymer\", \"dommarajugukesh\", \"rpragchess\"],\n",
    "        start_month=start_month,\n",
    "        end_month=end_month,\n",
    "    )\n",
    "    # load the \"players_games\" and \"players_profiles\" out of all the possible resources\n",
    "    info = pipeline.run(data.with_resources(\"players_games\", \"players_profiles\"))\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def load_players_online_status() -> None:\n",
    "    \"\"\"Constructs a pipeline that will append online status of selected players\"\"\"\n",
    "\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"chess_pipeline\",\n",
    "        destination=\"databricks\",\n",
    "        dataset_name=\"chess_players_games_data\",\n",
    "        staging=dlt.destinations.filesystem(\"az://jycontainer\")\n",
    "    )\n",
    "    data = source([\"magnuscarlsen\", \"vincentkeymer\", \"dommarajugukesh\", \"rpragchess\"])\n",
    "    info = pipeline.run(data.with_resources(\"players_online_status\"))\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def load_players_games_incrementally() -> None:\n",
    "    \"\"\"Pipeline will not load the same game archive twice\"\"\"\n",
    "    # loads games for 11.2022\n",
    "    load_players_games_example(\"2022/11\", \"2022/11\")\n",
    "    # second load skips games for 11.2022 but will load for 12.2022\n",
    "    load_players_games_example(\"2022/11\", \"2022/12\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run our main example\n",
    "    load_players_games_example(\"2022/11\", \"2022/12\")\n",
    "    #load_players_online_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No kernel connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to dlt-example-zHJnWXT8-py3.12 (Python 3.12.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71697dbd-001b-4693-9c17-0b9f454d5cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 13:50:31,062|[WARNING]|9012|139914589528192|dlt|pipeline.py|run:698|The pipeline `run` method will now load the pending load packages. The data you passed to the run function will not be loaded. In order to do that you must run the pipeline again\n",
      "2024-11-07 13:50:35,621|[ERROR]|9012|139912466654912|dlt|load.py|w_run_job:248|Terminal exception in job players_profiles.56d1fedefe.reference in file /home/jamesyou/.dlt/pipelines/chess_pipeline/load/normalized/1731015943.5909836/started_jobs/players_profiles.56d1fedefe.0.reference\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jamesyou/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/common/destination/reference.py\", line 420, in run_managed\n",
      "    self.run()\n",
      "  File \"/home/jamesyou/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/destinations/impl/databricks/databricks.py\", line 74, in run\n",
      "    raise LoadJobTerminalException(\n",
      "dlt.destinations.exceptions.LoadJobTerminalException: Job with id/file name /home/jamesyou/.dlt/pipelines/chess_pipeline/load/normalized/1731015943.5909836/started_jobs/players_profiles.56d1fedefe.0.reference encountered unrecoverable problem: Databricks cannot load data from staging bucket file:///home/jamesyou/dlt_example/dlt_data/chess_players_games_data/players_profiles/1731015943.5909836.56d1fedefe.parquet. Only s3, azure and gcs buckets are supported. Please note that gcs buckets are supported only via named credential\n",
      "2024-11-07 13:50:36,358|[ERROR]|9012|139914589528192|dlt|load.py|complete_jobs:413|Job for players_profiles.56d1fedefe.reference failed terminally in load 1731015943.5909836 with message Job with id/file name /home/jamesyou/.dlt/pipelines/chess_pipeline/load/normalized/1731015943.5909836/started_jobs/players_profiles.56d1fedefe.0.reference encountered unrecoverable problem: Databricks cannot load data from staging bucket file:///home/jamesyou/dlt_example/dlt_data/chess_players_games_data/players_profiles/1731015943.5909836.56d1fedefe.parquet. Only s3, azure and gcs buckets are supported. Please note that gcs buckets are supported only via named credential\n"
     ]
    },
    {
     "ename": "PipelineStepFailed",
     "evalue": "Pipeline execution failed at stage load with exception:\n\n<class 'dlt.load.exceptions.LoadClientJobFailed'>\nJob for players_profiles.56d1fedefe.reference failed terminally in load 1731015943.5909836 with message Job with id/file name /home/jamesyou/.dlt/pipelines/chess_pipeline/load/normalized/1731015943.5909836/started_jobs/players_profiles.56d1fedefe.0.reference encountered unrecoverable problem: Databricks cannot load data from staging bucket file:///home/jamesyou/dlt_example/dlt_data/chess_players_games_data/players_profiles/1731015943.5909836.56d1fedefe.parquet. Only s3, azure and gcs buckets are supported. Please note that gcs buckets are supported only via named credential. The package is aborted and cannot be retried.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLoadClientJobFailed\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:580\u001b[0m, in \u001b[0;36mPipeline.load\u001b[0;34m(self, destination, dataset_name, credentials, workers, raise_on_failed_jobs)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39mwith\u001b[39;00m signals\u001b[39m.\u001b[39mdelayed_signals():\n\u001b[0;32m--> 580\u001b[0m     runner\u001b[39m.\u001b[39;49mrun_pool(load_step\u001b[39m.\u001b[39;49mconfig, load_step)\n\u001b[1;32m    581\u001b[0m info: LoadInfo \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_step_info(load_step)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/common/runners/pool_runner.py:91\u001b[0m, in \u001b[0;36mrun_pool\u001b[0;34m(config, run_f)\u001b[0m\n\u001b[1;32m     90\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRunning pool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[39mwhile\u001b[39;00m _run_func():\n\u001b[1;32m     92\u001b[0m     \u001b[39m# for next run\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     signals\u001b[39m.\u001b[39mraise_if_signalled()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/common/runners/pool_runner.py:84\u001b[0m, in \u001b[0;36mrun_pool.<locals>._run_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(run_f, Runnable):\n\u001b[0;32m---> 84\u001b[0m     run_metrics \u001b[39m=\u001b[39m run_f\u001b[39m.\u001b[39;49mrun(cast(TExecutor, pool))\n\u001b[1;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/load/load.py:638\u001b[0m, in \u001b[0;36mLoad.run\u001b[0;34m(self, pool)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_info_start_load_id(load_id)\n\u001b[0;32m--> 638\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_single_package(load_id, schema)\n\u001b[1;32m    640\u001b[0m \u001b[39mreturn\u001b[39;00m TRunMetrics(\u001b[39mFalse\u001b[39;00m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_storage\u001b[39m.\u001b[39mlist_normalized_packages()))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/load/load.py:597\u001b[0m, in \u001b[0;36mLoad.load_single_package\u001b[0;34m(self, load_id, schema)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m pending_exception:\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mraise\u001b[39;00m pending_exception\n\u001b[1;32m    598\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mLoadClientJobFailed\u001b[0m: Job for players_profiles.56d1fedefe.reference failed terminally in load 1731015943.5909836 with message Job with id/file name /home/jamesyou/.dlt/pipelines/chess_pipeline/load/normalized/1731015943.5909836/started_jobs/players_profiles.56d1fedefe.0.reference encountered unrecoverable problem: Databricks cannot load data from staging bucket file:///home/jamesyou/dlt_example/dlt_data/chess_players_games_data/players_profiles/1731015943.5909836.56d1fedefe.parquet. Only s3, azure and gcs buckets are supported. Please note that gcs buckets are supported only via named credential. The package is aborted and cannot be retried.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m/home/jamesyou/dlt_example/chess_pipeline.py:50\u001b[0m\n\u001b[1;32m     45\u001b[0m     load_players_games_example(\u001b[39m\"\u001b[39m\u001b[39m2022/11\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2022/12\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[39m# run our main example\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     load_players_games_example(\u001b[39m\"\u001b[39;49m\u001b[39m2022/11\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m2022/12\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     51\u001b[0m     \u001b[39m#load_players_online_status()\u001b[39;00m\n",
      "File \u001b[1;32m/home/jamesyou/dlt_example/chess_pipeline.py:22\u001b[0m\n\u001b[1;32m     16\u001b[0m data \u001b[39m=\u001b[39m source(\n\u001b[1;32m     17\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39mmagnuscarlsen\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvincentkeymer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdommarajugukesh\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrpragchess\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m     start_month\u001b[39m=\u001b[39mstart_month,\n\u001b[1;32m     19\u001b[0m     end_month\u001b[39m=\u001b[39mend_month,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39m# load the \"players_games\" and \"players_profiles\" out of all the possible resources\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m info \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mrun(data\u001b[39m.\u001b[39;49mwith_resources(\u001b[39m\"\u001b[39;49m\u001b[39mplayers_games\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mplayers_profiles\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(info)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:227\u001b[0m, in \u001b[0;36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m trace:\n\u001b[1;32m    225\u001b[0m         trace_step \u001b[39m=\u001b[39m start_trace_step(trace, cast(TPipelineStep, f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m), \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m     step_info \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m step_info\n\u001b[1;32m    229\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:276\u001b[0m, in \u001b[0;36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    270\u001b[0m     \u001b[39m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39mwith\u001b[39;00m inject_section(\n\u001b[1;32m    272\u001b[0m         ConfigSectionContext(\n\u001b[1;32m    273\u001b[0m             pipeline_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline_name, sections\u001b[39m=\u001b[39msections, merge_style\u001b[39m=\u001b[39mmerge_func\n\u001b[1;32m    274\u001b[0m         )\n\u001b[1;32m    275\u001b[0m     ):\n\u001b[0;32m--> 276\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:703\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, data, destination, staging, dataset_name, credentials, table_name, write_disposition, columns, primary_key, schema, loader_file_format, table_format, schema_contract, refresh)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    698\u001b[0m         logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    699\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe pipeline `run` method will now load the pending load packages. The data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m you passed to the run function will not be loaded. In order to do that you\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m must run the pipeline again\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m         )\n\u001b[0;32m--> 703\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload(destination, dataset_name, credentials\u001b[39m=\u001b[39;49mcredentials)\n\u001b[1;32m    705\u001b[0m \u001b[39m# extract from the source\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:227\u001b[0m, in \u001b[0;36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m trace:\n\u001b[1;32m    225\u001b[0m         trace_step \u001b[39m=\u001b[39m start_trace_step(trace, cast(TPipelineStep, f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m), \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m     step_info \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m step_info\n\u001b[1;32m    229\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:167\u001b[0m, in \u001b[0;36mwith_state_sync.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m should_extract_state \u001b[39m=\u001b[39m may_extract_state \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mrestore_from_destination\n\u001b[1;32m    166\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_state(extract_state\u001b[39m=\u001b[39mshould_extract_state):\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:276\u001b[0m, in \u001b[0;36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    270\u001b[0m     \u001b[39m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39mwith\u001b[39;00m inject_section(\n\u001b[1;32m    272\u001b[0m         ConfigSectionContext(\n\u001b[1;32m    273\u001b[0m             pipeline_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline_name, sections\u001b[39m=\u001b[39msections, merge_style\u001b[39m=\u001b[39mmerge_func\n\u001b[1;32m    274\u001b[0m         )\n\u001b[1;32m    275\u001b[0m     ):\n\u001b[0;32m--> 276\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/dlt-example-zHJnWXT8-py3.12/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:587\u001b[0m, in \u001b[0;36mPipeline.load\u001b[0;34m(self, destination, dataset_name, credentials, workers, raise_on_failed_jobs)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m l_ex:\n\u001b[1;32m    586\u001b[0m     step_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_step_info(load_step)\n\u001b[0;32m--> 587\u001b[0m     \u001b[39mraise\u001b[39;00m PipelineStepFailed(\n\u001b[1;32m    588\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mload\u001b[39m\u001b[39m\"\u001b[39m, load_step\u001b[39m.\u001b[39mcurrent_load_id, l_ex, step_info\n\u001b[1;32m    589\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39ml_ex\u001b[39;00m\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m: Pipeline execution failed at stage load with exception:\n\n<class 'dlt.load.exceptions.LoadClientJobFailed'>\nJob for players_profiles.56d1fedefe.reference failed terminally in load 1731015943.5909836 with message Job with id/file name /home/jamesyou/.dlt/pipelines/chess_pipeline/load/normalized/1731015943.5909836/started_jobs/players_profiles.56d1fedefe.0.reference encountered unrecoverable problem: Databricks cannot load data from staging bucket file:///home/jamesyou/dlt_example/dlt_data/chess_players_games_data/players_profiles/1731015943.5909836.56d1fedefe.parquet. Only s3, azure and gcs buckets are supported. Please note that gcs buckets are supported only via named credential. The package is aborted and cannot be retried."
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "from chess import source\n",
    "\n",
    "\n",
    "def load_players_games_example(start_month: str, end_month: str) -> None:\n",
    "    \"\"\"Constructs a pipeline that will load chess games of specific players for a range of months.\"\"\"\n",
    "\n",
    "    # configure the pipeline: provide the destination and dataset name to which the data should go\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"chess_pipeline\",\n",
    "        destination=\"databricks\",\n",
    "        dataset_name=\"chess_players_games_data\",\n",
    "        staging=dlt.destinations.filesystem(\"dlt_data\")\n",
    "    )\n",
    "    # create the data source by providing a list of players and start/end month in YYYY/MM format\n",
    "    data = source(\n",
    "        [\"magnuscarlsen\", \"vincentkeymer\", \"dommarajugukesh\", \"rpragchess\"],\n",
    "        start_month=start_month,\n",
    "        end_month=end_month,\n",
    "    )\n",
    "    # load the \"players_games\" and \"players_profiles\" out of all the possible resources\n",
    "    info = pipeline.run(data.with_resources(\"players_games\", \"players_profiles\"))\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def load_players_online_status() -> None:\n",
    "    \"\"\"Constructs a pipeline that will append online status of selected players\"\"\"\n",
    "\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"chess_pipeline\",\n",
    "        destination=\"databricks\",\n",
    "        dataset_name=\"chess_players_games_data\",\n",
    "        staging=dlt.destinations.filesystem(\"dlt_data\")\n",
    "    )\n",
    "    data = source([\"magnuscarlsen\", \"vincentkeymer\", \"dommarajugukesh\", \"rpragchess\"])\n",
    "    info = pipeline.run(data.with_resources(\"players_online_status\"))\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def load_players_games_incrementally() -> None:\n",
    "    \"\"\"Pipeline will not load the same game archive twice\"\"\"\n",
    "    # loads games for 11.2022\n",
    "    load_players_games_example(\"2022/11\", \"2022/11\")\n",
    "    # second load skips games for 11.2022 but will load for 12.2022\n",
    "    load_players_games_example(\"2022/11\", \"2022/12\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run our main example\n",
    "    load_players_games_example(\"2022/11\", \"2022/12\")\n",
    "    #load_players_online_status()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlt-example-zHJnWXT8-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
